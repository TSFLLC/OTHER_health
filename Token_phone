USE TokenizationDB;
GO

-- 1. Drop the old uniqueness on TokenValue
IF EXISTS (
    SELECT 1
    FROM sys.indexes
    WHERE name = 'UQ_Token_Per_Column'
      AND object_id = OBJECT_ID('dbo.Token_Map')
)
BEGIN
    ALTER TABLE dbo.Token_Map
    DROP CONSTRAINT UQ_Token_Per_Column;
END
GO

-- 2. Enforce uniqueness per ORIGINAL value instead
IF NOT EXISTS (
    SELECT 1
    FROM sys.indexes
    WHERE name = 'UX_TokenMap_OrigVal'
      AND object_id = OBJECT_ID('dbo.Token_Map')
)
BEGIN
    CREATE UNIQUE INDEX UX_TokenMap_OrigVal
    ON dbo.Token_Map(TableSchema, TableName, ColumnName, TokenType, OriginalValue);
END
GO





----------------------------Update usp_GenerateStringToken  ------------------------------------------------------

USE TokenizationDB;
GO

IF OBJECT_ID('dbo.usp_GenerateStringToken', 'P') IS NOT NULL
    DROP PROCEDURE dbo.usp_GenerateStringToken;
GO

CREATE PROCEDURE dbo.usp_GenerateStringToken
    @TableSchema     SYSNAME,
    @TableName       SYSNAME,
    @ColumnName      SYSNAME,
    @TokenType       VARCHAR(50),
    @OriginalValue   NVARCHAR(400),
    @MaxTokenLength  INT,
    @TokenValue      NVARCHAR(100) OUTPUT
AS
BEGIN
    SET NOCOUNT ON;

    IF @OriginalValue IS NULL
    BEGIN
        SET @TokenValue = NULL;
        RETURN;
    END;

    IF @MaxTokenLength IS NULL OR @MaxTokenLength <= 0
    BEGIN
        RAISERROR('usp_GenerateStringToken: MaxTokenLength must be > 0.',16,1);
        RETURN;
    END;

    -------------------------------------------------
    -- 1) Reuse existing mapping if it already exists
    -------------------------------------------------
    SELECT @TokenValue = TokenValue
    FROM dbo.Token_Map
    WHERE TableSchema   = @TableSchema
      AND TableName     = @TableName
      AND ColumnName    = @ColumnName
      AND TokenType     = @TokenType
      AND OriginalValue = @OriginalValue;

    IF @TokenValue IS NOT NULL
        RETURN;    -- mapping already exists, nothing more to do

    -------------------------------------------------
    -- 2) Generate one new format-preserving token
    -------------------------------------------------
    DECLARE 
        @NewToken NVARCHAR(100),
        @Len      INT,
        @i        INT,
        @ch       NCHAR(1),
        @code     INT,
        @r        INT;

    SET @Len = LEN(@OriginalValue);
    IF @Len > @MaxTokenLength SET @Len = @MaxTokenLength;

    SET @NewToken = N'';
    SET @i = 1;

    WHILE @i <= @Len
    BEGIN
        SET @ch   = SUBSTRING(@OriginalValue, @i, 1);
        SET @code = UNICODE(@ch);

        -- digits 0â€“9
        IF @code BETWEEN 48 AND 57
        BEGIN
            SET @r = ABS(CHECKSUM(NEWID())) % 10;
            SET @NewToken += NCHAR(48 + @r);
        END
        -- uppercase Aâ€“Z
        ELSE IF @code BETWEEN 65 AND 90
        BEGIN
            SET @r = ABS(CHECKSUM(NEWID())) % 26;
            SET @NewToken += NCHAR(65 + @r);
        END
        -- lowercase aâ€“z
        ELSE IF @code BETWEEN 97 AND 122
        BEGIN
            SET @r = ABS(CHECKSUM(NEWID())) % 26;
            SET @NewToken += NCHAR(97 + @r);
        END
        -- keep punctuation / spaces
        ELSE
        BEGIN
            SET @NewToken += @ch;
        END;

        SET @i += 1;
    END;

    -- 3) Store mapping (unique by OriginalValue)
    INSERT INTO dbo.Token_Map
        (TableSchema, TableName, ColumnName, TokenType, TokenValue, OriginalValue)
    VALUES
        (@TableSchema, @TableName, @ColumnName, @TokenType, @NewToken, @OriginalValue);

    SET @TokenValue = @NewToken;
END;
GO


------What is happening.
With 10,000+ rows and a limited pattern (e.g., short phone field, a lot of similar values), you eventually run out of new tokens or keep colliding â†’ 100 attempts â†’ error.


FORMAT-PRESERVING tokenization does NOT work for datetime

â€œconversion failed when converting date/time from character stringâ€

ðŸ”§ Available solutions

I can implement any of these, depending on the requirement:

Option A â€” Convert DOB to nvarchar and tokenize like a string

(Easiest but changes data type)

Option B â€” Generate valid fake DOBs

Example: keep same year Â± random offset
or keep age group
or entirely new random but valid dates.

Option C â€” Remove DOB from tokenization

(You already chose this for POC)

Option D â€” Build a special DOB-token generator

That outputs valid datetime tokens.

If you want, I can write a DOB masking function that:

keeps it a valid datetime

avoids duplicates

preserves age group

or keeps year only

Just tell me what business rule you want for DOB.


--------------------------+++++++++++++++++++---------------------------

SELECT 
    OriginalValue,
    TokenValue,
    COUNT(*) AS RowCount
FROM TokenizationDB.dbo.Token_Map
WHERE TableSchema = 'B_iam'
  AND TableName   = 'srcAya'
  AND ColumnName  = 'Phone'
GROUP BY OriginalValue, TokenValue
ORDER BY OriginalValue;



++++++++++++++++++++++++
SELECT TOP 10 OriginalValue, TokenValue
FROM TokenizationDB.dbo.Token_Map
WHERE TableSchema = 'B_iam'
  AND TableName   = 'srcAya'
  AND ColumnName  = 'Phone';


+++++++++++++++++++++++++++++++++++++++++++++new+++++++++++++++++++++++++++++++++

USE Masking;
GO

IF OBJECT_ID('dbo.usp_TokenizeColumn_RowWise', 'P') IS NOT NULL
    DROP PROCEDURE dbo.usp_TokenizeColumn_RowWise;
GO

CREATE PROCEDURE dbo.usp_TokenizeColumn_RowWise
    @SchemaName   SYSNAME,
    @TableName    SYSNAME,
    @ColumnName   SYSNAME,
    @TokenType    VARCHAR(50),
    @KeyColumn    SYSNAME        -- e.g. 'Employee ID'
AS
BEGIN
    SET NOCOUNT ON;

    DECLARE 
        @sql      NVARCHAR(MAX),
        @MaxLen   INT,
        @DataType SYSNAME;

    -------------------------------------------------
    -- 1) Get datatype and length of column to tokenize
    -------------------------------------------------
    SELECT 
        @DataType = t.name,
        @MaxLen   = CASE 
                        WHEN t.name IN ('nvarchar','nchar')
                             THEN c.max_length / 2
                        ELSE c.max_length
                    END
    FROM sys.columns c
    JOIN sys.types   t ON c.user_type_id = t.user_type_id
    JOIN sys.objects o ON c.object_id    = o.object_id
    WHERE o.name = @TableName
      AND SCHEMA_NAME(o.schema_id) = @SchemaName
      AND c.name = @ColumnName;

    IF @MaxLen IS NULL
    BEGIN
        RAISERROR('usp_TokenizeColumn_RowWise: column not found.',16,1);
        RETURN;
    END;

    IF @DataType NOT IN ('char','nchar','varchar','nvarchar')
    BEGIN
        RAISERROR('usp_TokenizeColumn_RowWise: only char/varchar/nchar/nvarchar supported.',16,1);
        RETURN;
    END;

    -------------------------------------------------
    -- 2) Temp table with Key + OriginalValue
    -------------------------------------------------
    IF OBJECT_ID('tempdb..#RowsToTokenize') IS NOT NULL
        DROP TABLE #RowsToTokenize;

    CREATE TABLE #RowsToTokenize
    (
        KeyValue NVARCHAR(400) NOT NULL,
        Value    NVARCHAR(400) NOT NULL
    );

    SET @sql = N'
        INSERT INTO #RowsToTokenize (KeyValue, Value)
        SELECT CONVERT(NVARCHAR(400), ' + QUOTENAME(@KeyColumn) + N'),
               CAST(' + QUOTENAME(@ColumnName) + N' AS NVARCHAR(400))
        FROM ' + QUOTENAME(@SchemaName) + N'.' + QUOTENAME(@TableName) + N'
        WHERE ' + QUOTENAME(@ColumnName) + N' IS NOT NULL;
    ';

    EXEC sp_executesql @sql;

    -------------------------------------------------
    -- 3) Loop rows and tokenize via TokenizationDB
    -------------------------------------------------
    DECLARE 
        @KeyValue      NVARCHAR(400),
        @OriginalValue NVARCHAR(400),
        @TokenValue    NVARCHAR(100);

    DECLARE cur CURSOR FAST_FORWARD FOR
        SELECT KeyValue, Value FROM #RowsToTokenize;

    OPEN cur;
    FETCH NEXT FROM cur INTO @KeyValue, @OriginalValue;

    WHILE @@FETCH_STATUS = 0
    BEGIN
        EXEC TokenizationDB.dbo.usp_GenerateStringToken
            @TableSchema    = @SchemaName,
            @TableName      = @TableName,
            @ColumnName     = @ColumnName,
            @TokenType      = @TokenType,
            @OriginalValue  = @OriginalValue,
            @MaxTokenLength = @MaxLen,
            @TokenValue     = @TokenValue OUTPUT;

        SET @sql = N'
            UPDATE t
            SET ' + QUOTENAME(@ColumnName) + N' = @TokenValue
            FROM ' + QUOTENAME(@SchemaName) + N'.' + QUOTENAME(@TableName) + N' t
            WHERE CONVERT(NVARCHAR(400), t.' + QUOTENAME(@KeyColumn) + N') = @KeyValue;
        ';

        EXEC sp_executesql
            @sql,
            N'@TokenValue NVARCHAR(100), @KeyValue NVARCHAR(400)',
            @TokenValue = @TokenValue,
            @KeyValue   = @KeyValue;

        FETCH NEXT FROM cur INTO @KeyValue, @OriginalValue;
    END;

    CLOSE cur;
    DEALLOCATE cur;

    DROP TABLE #RowsToTokenize;
END;
GO


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
SELECT 
    COUNT(*)                            AS TotalMappings,
    COUNT(DISTINCT OriginalValue)       AS DistinctOriginalValues,
    COUNT(DISTINCT TokenValue)          AS DistinctTokenValues
FROM TokenizationDB.dbo.Token_Map
WHERE TableSchema = 'B_iam'
  AND TableName   = 'srcAya'
  AND ColumnName  = 'SSN'
  AND TokenType   = 'SSN';

SELECT TOP 20 OriginalValue, TokenValue
FROM TokenizationDB.dbo.Token_Map
WHERE TableSchema = 'B_iam'
  AND TableName   = 'srcAya'
  AND ColumnName  = 'SSN'
  AND TokenType   = 'SSN'
ORDER BY OriginalValue;





